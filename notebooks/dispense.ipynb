{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b5a7f0-5af9-4a8b-a108-02a24b69625e",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "## Jacopo Omodei e Michelangelo Leoni\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc687a8-0d77-412d-9d18-8b5c6c96829e",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "[Intro](#intro)\n",
    "\n",
    "[Linear](#linear)\n",
    "\n",
    "[Knn](#knn)\n",
    "\n",
    "[Neural Networks](#NN)\n",
    " - [Back-propagation](#BPP)\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c7cc22b-61fc-4912-8451-195684dea0d2",
   "metadata": {},
   "source": [
    "# Introduction (lezioni 1-4)\n",
    "\n",
    "<a id=\"intro\"></a>\n",
    "\n",
    "Loss: how do we measure the quality of the approximation?\n",
    "We produce an h(x) value (model evaluarted at input point x), and we want to measure the distance between h(x) and d (known input target for x). \n",
    "We use an \"inner\" loss \\$L(h_w(x),d)$, which we would like to be small. \n",
    "\n",
    "The _Error_ (or _Risk_ or _Loss_) is an expected value of this _L_ (a \"sum\" or mean of the inner loss _L_ over the set of samples)\n",
    "\\$Loss(h_w)=E_{rror}(w)=\\frac{1}{l}\\sum^l_{p=1}L(h_w(x_p),d_p)$\n",
    "\n",
    "Def: __Regression__: _predicting a numerical value_\n",
    "- __Oputput__: \\$d_p=f(x_p)+e$ (real value function + random error)\n",
    "- __H__: a set of real-valued functions\n",
    "- __Loss function__ _L_: measures the approximation accuraccy/error\n",
    "- A common loss function for regression: the squared error:\n",
    "\n",
    "\\$L(h_w(x_p),d_p)=(d_p-h_w(x_p))^2$\n",
    "The mean over the dataset provides the MSE\n",
    "\n",
    "Def: __Classification__: slide 57 2-3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Loss:\n",
    "Loss function: funzione dentro\n",
    "\n",
    "Error: somma sui dati della loss function\n",
    "\n",
    "Loss: Il valore di aspettazione della loss function (Error / numero data). Ci possono essere altri termini aggiunti che vogliamo minimizzare ma che non sono da considerare come errori sui dati. Essendo la funzione che voglio minimizzare, è usata durante il training, mentre l'errore per quando vuoi valutare come va il modello.\n",
    "\n",
    "Empirical risk (what we minimize, empirical risk minimization ERM), also called _training error E_: \\$R_{emp}=R_{emp}(h,TR)=\\frac{1}{l}\\sum^l_{p=1}L(h_w(x_p),d_p)$. \n",
    "\n",
    "The idea is that we are using \\$R_{emp}$ to approximate R\n",
    "\n",
    "## Vapnik-Chervonenkis-dim and SLT:\n",
    "- Given the _VC-dim_, a measure of the complexity of _H_ (flexibility to fit data)\n",
    "\n",
    "\\$R \\leq R_{emp} + \\varepsilon(1/l,dim(VC),1/\\delta)$\n",
    "\n",
    "where l is the dimension of the dataset, and delta is the confidence level. The term that is added to the empirical R (so, epsilon) is called the VC-confidence.\n",
    "\n",
    "![Vapnik-Chervonenkis_diagram](figures/Vapnik-Chervonenkis_diagram.png)\n",
    "\n",
    "__SLT__: Statistical learning theory. It allows formal framing of the problem of generalization and overfitting, providing \n",
    "analytic upper-bound  to the risk R for the prediction over all the data, regardless to \n",
    "the type of learning algorithm or details of the model\n",
    "\n",
    "## Validation\n",
    "After the model has trained you can use it to evaluate how good the model was on the validation data (to make choices about model selection) or to estimate its prediction/generalization error on new test data. Either way, it returns an estimation\n",
    "\n",
    "Partition data set D into three parts. _training set_ TR, _validation/selection set_ VL, _test set_ TS.\n",
    "- All three of the sets are disjointed and serve their own function\n",
    "- __TR__ is used for the training algorithm\n",
    "- __VL__ is used to select the best model (like tuning hyper-parameters)\n",
    "- __TS__ is not to be used for tuning but instead to assess the model.\n",
    "\n",
    "__K-fold__ cross validation: we will see in the future __LINK__\n",
    "\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0aacce-421c-48e8-9c91-d3d135e87537",
   "metadata": {},
   "source": [
    "# Linear models (L5)\n",
    "<a id=\"linear\"></a>\n",
    "\n",
    "A linear model is a model which the input vector is weighted with linear coefficient weights:\n",
    "\\$w^Tx+w_0 = w_0 + \\sum_{i=1}^nw_ix_i$\n",
    "\n",
    "\\$w_0$ is the _intercept, threshold, bias, offset..._ Often it is convenient to include the constant x_0=1 so that we can encapsulate the bias w_0 as the first element of the vector w. Like this we get:\n",
    "\n",
    "\\$h(x_p)=x_p^Tw=\\sum_{i=0}^nw_ix_i$\n",
    "\n",
    "_w_ are the free parameters, the \"weights\".\n",
    "\n",
    "Given the data set and the linear model, we can state the learning problem as a LMS problem. We just have to find the best weights so that we have the best \\$h_w(x)$\n",
    "\n",
    "#### Bias\n",
    "- Language bias: the H is a set of linear functions (restrictive)\n",
    "- Ordered search guided by the Least Squares minimization goal \n",
    "\n",
    "## Classification\n",
    "The function \\$h(x_p)=x_p^Tw=\\sum_{i=0}^nw_ix_i$ rappresents a hyperplane in n dimensions that we can use to split our space into a positive and a negative space. __LTU__ (Linear threshold unit) gives the seperation between these two zones. In general, two groups are linearly separable in n-dimensional space if they can be separated by an (n − 1)-dimensional hyperplane, and an exact solution exists only for linearly separable groups.\n",
    "\n",
    "Tecnically it would be intuitive, at least for classification problems, to use a loss function that is equal to 1 for misclassified data and 0 for correctly classified data. However, this loss function is non-differentiable and if we want to implement gradient descent minimization we need a smooth loss function, like the squared errors loss function. A closed form solution exists, but we will opt for a recursive gradient descent technique.\n",
    "\n",
    "Binary classification loss functions (0 if classified correctly, 1 if wrong) are not useless, quite the opposite! You can't use them to train the machine as it is not differentiable, but once trained you can calculate it to see then number of errors that your model commits. We define _accuracy_ the mean of correctly classified errors \\$\\frac{l-N_{err}}{l}$.\n",
    "\n",
    "__Gradient descent technique__\n",
    "We calculate the gradient of the loss which gives us the direction to change each weight in order to decrease the loss. Therefore:\n",
    "\\$w_{new} = w + \\eta\\cdot \\Delta w \\quad \\text{with} \\quad \\Delta w = - \\frac{\\partial E(W)}{\\partial w}$\n",
    "\n",
    "\\$\\eta$ is the learning rate (step-size)\n",
    "\n",
    "### Batch vs On-line (SGD) gradient descent\n",
    "What has been discussed before is the batch gradient descent, where the loss is calculated and the gradient is taken after one epoch (all of the data has been used) has passed. Then, the gradient is evaluated and the weights are modified before the next full _batch_. \n",
    "\n",
    "The alternative is the SGD (Stochastic Gradient Descent), where after every data point the loss function is calculated and the weights are modified - so that in one epoch you have many many adjustments to the model. This can be faster, as it does not wait for a full epoch to pass before modifying _w_, but the direction of the modification depends only on the data that has already been seen. In the figure, blue is the batch descent, purple SGD and green is a mini-batch SGD (not on every point but modulated)\n",
    "\n",
    "![Batch vs SGD](figures/batch_vs_SGD.png)\n",
    "\n",
    "## Extension of the linear model and generalization\n",
    "Let's remember that for a model to be considered linear it must be linear in the parameters (w). For example, a totally legit linear model is the _polynomial regression_: \\$h_w(x) = w_0 + w_1\\cdot x + w_2\\cdot x^2 + \\cdots + w_n\\cdot x^n = \\sum_{i=0}^nw_ix^i$\n",
    "\n",
    "__Linear basis expansion (LBE)__: \\$h_w(x)=\\sum_{k=0}^Kw_k\\phi_k(x)$. \n",
    "\n",
    "The phi functions are generic, and so we have the usual sum of \\$w_ix_i$ for the _n_-dimension of the space, plus potential other _w's_ multiplied by functions of the input data. Therefore, in general, _K_ is greater than _n_\n",
    "\n",
    "This is fantastic because it allows us more flexibility in our model but risks the _curse of dimensionality_, where the volume of the problem space increases so fast that the available data become sparse, the data became no more sufficient to support the model complexity. Also, the \\$\\phi$ are fixed before observing the training data (otherwise it would be an adaptive model like a NN).\n",
    "\n",
    "__Ridge regression: (Tikhonov regularization):__\n",
    "There is a way to combat the  _curse of dimensionality_, penalizing models with high values of _|w|_. I add a term that is proportional to the modulus squared (\\$ ||w||^2=\\sum w_j^2$) to the loss. \n",
    "\n",
    "\\$Loss(w) = \\sum_{p=1}^l(y_p-x_p^Tw)^2+\\lambda ||w||^2$. Lambda (regulatization hyper-parameter) is a small positive value. The second term of this loss function is called the regularization (penalty) term.\n",
    "\n",
    "In the gradient descent formularization, we get \\$w_{new} = w + \\eta\\cdot \\Delta w - 2 \\lambda \\cdot w$. As we can see, it is a _weight decay technique, which \"pulls\" all of the w's towards 0. Since you are reducing the weights overall, you are also simplyifing the model (reducing the VC-dim).\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4b876-4821-4c8a-a1bf-03f452512900",
   "metadata": {},
   "source": [
    "# KNN (L6)\n",
    "<a id=\"knn\"></a>\n",
    "\n",
    "__Timing__\n",
    "- Eager: Analyze the training data and construct an explicit hypothesis\n",
    "- Lazy: Store the training data and wait until a test data point is presented, then construct an ad hoc hypothesis to classify that one data point.\n",
    "\n",
    "So far we have seen eager models, where the full \\$h(x)$ was found by looking at all of the data, wheras k-nn finds the best classification point by point.  \n",
    "\n",
    "## k-nn algorithm:\n",
    "- Store the training data\n",
    "- Given an input x find the nearest training example \\$x_i\\$\n",
    "- Then output \\$y_i$\n",
    "\n",
    "Find _i_ so that we have \\$\\min d(x,x_i) \\rightarrow i(x) = arg_p \\min d(x,x_p) \\quad \\text{using Euclidean distance} \\quad d(x,x_p)=\\sqrt{\\sum_{t=1}^n(x_t-x_{p,t})^2}=||x-x_p||$ \n",
    "\n",
    "k-nn refers to how many neighbors are considered in the nearest neighbor evaluation. For example, the one above is a 1-nn algorithm.\n",
    "\n",
    "A natural way to classify a new point is to have a look at its k-neighbors and take the average. \n",
    "\n",
    "If there is a clear dominance of one of the classes in the neighborhood of an observation x, then it is likely that the observation itself would belong to that class, too. Thus the classification rule is the majority voting among the members of that neighborhood. This extends naturally to multi-class classification where you look at the class of the majority of your neighbors. \n",
    "\n",
    "You can also create an algorithm that gives more weight in the nearest neighbor calculation to the closest point. For example, you can multiply by a factor \\$ \\propto \\frac{1}{d^2}\\$. \n",
    "\n",
    "## Algorithm flexibility\n",
    "\n",
    "The k-nn algorithm has _k_ parameters, but \\$\\frac{l}{k}\\$ _effective degrees of freedom_. This number is roughly equal to the number of k-point neighborhoods and can be used to estimate the flexibility of the model. If _k_ is 1, then teh model has no training error, and is in _overfitting_. For \\$k=l\\$, the model is obviously in _underfitting_ since the model outputs a constant value for all inputs. \n",
    "\n",
    "![k-nn parameters](figures/knn_parameters.png)\n",
    "\n",
    "The red curves are test and the green are training error for k-nn classification (changing K). The results for linear regression are the bigger green and red squares at three degrees of freedom. The purple line is the _optimal Bayes Error Rate_.\n",
    "\n",
    "__Bayes error rate__:\n",
    "Bayes optimal solution (called Bayes classifier): the minimum achievable error rate given the distribution of the data. Only calculatable if you know the initial distribution. K-nn models approach this limit quite well for a certain range of _k's_.\n",
    "\n",
    "__Inductive bias__: type of distance used (Euclidean, etc.) \n",
    "\n",
    "Limitations: \n",
    "\n",
    "-  Computationally intensive (in time) for each new input: computing the distances from the test sample to _all_ stored vectors local approximations and in space because you must memorize the full training set.\n",
    "- Curse of Dimensionality: increasing the dimension causes a loss of density in the training space. If you need _l_ points to sufficiently sample a 1-dimensional space, then you will need \\$l^n\\$ points to obtain a similar sampling density in an _n-dimensional_ space.\n",
    "- Curse of Noisy:  if the target depends on only few of many features in x we could retrieve a “similar pattern” with the similarity  dominated by the large number of irrelevant features.\n",
    "- Different data ranges correspond to certain features being favored.\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e84173-c1a7-44c1-980e-c279d65f5390",
   "metadata": {},
   "source": [
    "# Neural Networks (L7-9)\n",
    "<a id=\"NN\"></a>\n",
    "\n",
    "## Basic idea and definitions:\n",
    "Complex behavior emerging from interaction of simple computational units. \n",
    "\n",
    "A __node__ (or neuron, unit) takes inputs from external sources or other units. Each input connection has a certain __weight__ _w_, which will be the free parameters we will modify throughout the learning process (synaptic strength in real neurons).\n",
    "\n",
    "The weighted sum \\$net_i(x)=\\sum_jw_{ij}x_j\\$ is called the __net input__ to unit _i_. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \\(w_{ij}\\) refers to the weight of the input j onto the unit i\n",
    "</div>\n",
    "\n",
    "The function _f_ is the unit's __activation function__, and the output of the unit _i_ is \\$o_i(x)=f(net_i(x))\\$ \n",
    "\n",
    "![neuron_diagram](figures/neuron_diagram.png)\n",
    "\n",
    "## Activation functions:\n",
    "\n",
    "- Linear: \\$o_i(x)= \\sum_iw_ix_i\\$, so the output is just the net input.\n",
    "- Threshold: \\$o_i=sign(net_i(x))\\$. The unit either has an output or no output. \n",
    "- Sigmoid: \\$\\frac{1}{1+e^{-net_i(x)}}\\$ _sigmoid_ logistic function (a smoother 0-1 activation)\n",
    "- Hyperbolic tangent: Similar to sigmoid but between [-1,1]\n",
    "- LTU (Linear threshold unit): 0 for \\$x<0\\$ and linear for \\$x>0\\$.\n",
    "\n",
    "## Perceptron:\n",
    "Frank Rosenblatt (1957), biologically inspired model for a neural network, where every unit is very simple and the connections between them produce a more complex outcome\n",
    "\n",
    "__History: McCulloch & Pitts Networks:__\n",
    "Neurons are in two possible states: 1 or 0. All connections are equivalent and characterized by a real number (w). A neuron becomes active when the weighted sum of the connections + a bias is greater than 0 (threshold activation function). Therefore, each node can give a binary output, and so it is useful for binary classification tasks.\n",
    "\n",
    "With only two levels of interconnected nodes you can represent any Boolean function. With enough layers, perceptrons can solve any linearly seperatable problem. \n",
    "\n",
    "__Perceptron convergence theorem__: The perceptron is guaranteed to converge (classifying all the input patterns correctly) in a finite number of steps if the problem is linearly separable independently of the starting point. The final solution is not unique and it depends on the starting point. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The lecture [slides](../slides/L7_Neural_Networks.pdf)\n",
    " present a thorough proof of this theorem, which will not be treated in this document\n",
    "</div>\n",
    "\n",
    "No sort of learning algorithm proposed yet, just a way to solve a problem using principles inspired by nature. \n",
    "\n",
    "## Learning algorithms:\n",
    "### Perceptron learning algorithm:\n",
    "We have non-linear units during training, with a hard limiter on threshold activation function. It is only useful for classification studies. \n",
    "\n",
    "Its goal is to __minimize the number of misclassified patterns__ (while, for example, a LMS-based algorithm wants to minimize a certain loss). \n",
    "\n",
    "1.  Initialize the weights (either to zero or to a small random value)\n",
    "2.  Pick a learning rate \\$\\eta\\$ between 0 and 1\n",
    "3.  Loop until stopping condition (weights don't change - converged to a solution) is met\n",
    "\n",
    "For each training pattern (x,d), where d is +1 or -1\n",
    "\n",
    "- Compute the activation function \\$ out = sign(w^Tx) \\$ (equal to \\$ \\pm1 \\$)\n",
    "- If \\$out = d\\$ don't change the weights\n",
    "- If \\$out \\neq d\\$ update the weights \\$ w_{new} = w + \\frac{\\eta}{2} \\cdot (d-out)x\\$ (_Hebbian learning_) where \\$\\delta = (d-out)\\$\n",
    "\n",
    "This is an error correction rule, where the weights are updated proportionally to the error.\n",
    "\n",
    "### Sigmoid-logistic learning algorithm:\n",
    "We start by looking at what the __sigmoid activation function__ is. It is also called the _logistic activation function_ \\$f_\\sigma(x)=\\frac{1}{1+e^{-ax}}\\$ which is returns a continuoius and differentiable output between 0 and 1. You can tune the parameter _a_ which controls how sharply the function behaves around 0. If a tends to 0, the logistic function tends to the linear, if a tends to infinity, it tends to the threshold function.\n",
    "\n",
    "Similarly, you can use the hyperbolic tangent function: \\$f_{symm}=2f_\\sigma(x)-1=tanh(\\frac{ax}{2})\\$\n",
    "![sigmoid_and_tan](figures/sigmoid_tanh.png)\n",
    "For the sigmoid, you classify based on whether the output is greater than or less than 0.5. You can also define a _rejection zone_ near 0.5 where the output is \"I don't know\" for fragile decisions. The same goes for the tanh function around 0.\n",
    "\n",
    "Since the sigmoidal-logistic function has the property of being differentiable threshold function, we can derive a least mean square algorithm from it. Where before we had \\$o(x)=x^Tw\\$ now we have \\$o(x)=f_{sigma}(x^Tw)\\$ where \\$f_{sigma}\\$ is a logistic function. As before, we find _w_ to minimize the residual sum of squares: \\$E(w)=\\sum_p(d_p-o(x_p))^2=\\sum_p(d_p-f_\\sigma(x_p^Tw))^2\\$\n",
    "\n",
    "To obtain the gradient descent algorithm, you must take the derivative with respect to the weights of the error function. For a singular pattern, this constitues \n",
    "\n",
    "\\$\\frac{\\partial E(w)}{\\partial (w_j)} = -2\\sum_{p=1}^l (d_p-o(x_p))f^\\prime_\\sigma x_{p,j} \\$ where the new \\$\\delta_p = (d_p-o(x_p))f^\\prime_\\sigma$. The gradient descent algorithm remains the same as the linear unit just using the _new delta rule_: \\$w_{new}=w+\\eta \\cdot \\delta_p \\cdot x_p\\$\n",
    "\n",
    "Again, seen as an error correction rule. \n",
    "\n",
    "## The Neural Network (NN)\n",
    "### Standard feedforwanrd NN (with one hidden layer)\n",
    "\n",
    "In a __MLP__ (Multi-Layer Perceptron) architecture:\n",
    "- The units ar connecteed by __weighted links__ and theey are organized in the form of __layers__\n",
    "- The __input layer__ is simply the source of the input _x_ that projects onto the hidden layer of units. It loads (copies) the external input patterns _x_ without computing any function.\n",
    "- The __hidden layer__ projects onto the __output layer__ or to another hidden layer.\n",
    "![network_of_units](figures/network_of_units.png)\n",
    "\n",
    "Mathematically, this two-layer feedforward neural network in the figure can be seen as the following mathematical sum: \\$h(x)=f_k\\left( \\sum_jw_{kj}f_j\\left( \\sum_iw_{ji}x_i\\right)\\right)\\$\n",
    "\n",
    "Neural networks are classified by their __architecture__: which includes number of units, activation functions, number of layers (topology) and the learning algorithm used. Naturally, they are not linear in the parameters w. We can see \\$f_j\\left( \\sum_iw_{ji}x_i\\right)\\$ as a \\$\\phi_j(x,w)\\$ function so that the neural network mathematical form is similar to the linear basis function [(LBE)](#linear). Note, however, that this \\$\\phi\\$ depends on _w_ so the NN function is not linear in the parameters. Also, in the LBE theory the \\$\\phi's\\$ are fixed beforehand and they do not change with the training data. In this outlook, it's as if our basis functions (\\$\\phi\\$) are flexible, as they change with _w_.\n",
    "\n",
    "\n",
    "__Notation:__ For each unit _t_, the index _u_ denotes a generic input component. As usual, _x_ is the input vector. If we load the pattern _x_ in the input layer, we can use the notation with _o_ for all inputs. Therefore, putting it all together: the input to each unit _t_ from any source _u_ (through the connection \\$w_{tu}\\$) is typically denoted ad \\$o_u\\$. \n",
    "\n",
    "\n",
    "__Expressive power:__ The _expressive power_ is influenced by the number of units and the architecture. Specifically, influenced by the number of weights _w_ (proportional to the number of units). The higher the value of the weights, the more complex the model (the higher the VC-dim). \n",
    "\n",
    "\n",
    "### Feedforward vs recurrent NN\n",
    "__Feedforward:__ direction: input towards output\n",
    "\n",
    "For each input pattern _x_:\n",
    "1. The input pattern is loaded in the input layer\n",
    "2. The outputs of all of the units in the 1st hidden layer is computed\n",
    "3. Repeat for all hidden layers\n",
    "4. Compute the output for all units in the output layer\n",
    "5. We can now compute the error (delta) at the output level\n",
    "\n",
    "__Recurrent:__ based on feedback loops in the network topology. There can be loops to to other nodes behind the starting unit, or self-loops, where the model contains some memory of past computations. This allows us to process sequences (like text) and structured data.\n",
    "\n",
    "### NN outputs\n",
    "With one output, Neural Networks can deal with __classification tasks__ (sigmoidal output), or __regression tasks__ (linear output), Wuth multiple output units, we can deal with __multi-regression__ or __multi-class classification__ tasks. \n",
    "\n",
    "__Universal approximation theorem:__ A single hidden-layer network with logistic activation functions can approximate arbitrarily well every continuous function (provided enough units in the hidden layer). \n",
    "\n",
    "A MLP network can approxximate arbitrarily well every input-output mapping (provided enough units in the hidden layer). \n",
    "\n",
    "This theorem only states the existence, the practicality, algorithm and architecture are obviously not given by this theorem, that is the part where the Machine Learning programmer must make some decisions using his knowledge.\n",
    "\n",
    "-------\n",
    "\n",
    "# NN Part 2 (L9)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd3470-881c-4fa1-9004-0fe9a279b248",
   "metadata": {},
   "source": [
    "# Back-propagation (L8)\n",
    "<a id=\"BPP\"></a>\n",
    "As we said, the learning algorithm allows adapting the free parameters _w_ to obtain the best approximation of the target function, minimizing a loss on the training set. The difference now is that our model is not linear in the weights, and so there is not an obvious way to see which weight to change to better the output. \n",
    "\n",
    "As before, we need a differentiable loss, a differentiable activation function, but now we need a network to follow the information flow, to see which unit weight to modify.\n",
    "\n",
    "For the back-propagation calculation I will follow these [notes](../slides/L8_Back_Propagation.pdf).\n",
    "\n",
    "The idea is to estimate the contribution of hidden units to the error at the output level. \n",
    "\n",
    "![architecture](figures/backprop.png)\n",
    "\n",
    "### Notation:\n",
    "\n",
    "Each unit will be named _o_ with an index attached to it. The index can be _i, j, k_\n",
    "- \\$o_i\\$ are the input units where each input unit \\$o_i\\$ is loaded with the _i'th_ element of the input vector \\$x_i\\$. There are _l_ input patterns \\$x^p\\$ with \\$p\\in[1,l]\\$ (the index up means which pattern, the index down means which element of that pattern)\n",
    "- \\$o_j\\$ are the hidden layer units. There can be as many hidden layers as one wants but in this demonstration they will arbitrarily take the index _j_\n",
    "- \\$o_k\\$ are the output units, \\$k\\in[1,K]\\$, where _K_ is the dimension of the output\n",
    "\n",
    "Furthermore, the connection from unit _a_ to unit _b_ (with _a_, _b_ generic from _i,j,k_) is \\$w_{ba}\\$\n",
    "\n",
    "We will also use the concept of net input \\$net_t=\\sum_sw_{ts}o_s\\$ \n",
    "\n",
    "\n",
    "### Algorithm:\n",
    "\n",
    "We want to compute the error function (loss), compute the the gradient, and update all the weights _w_ in the network, until I have the desired convergence. \n",
    "I calculate the gradient \\$-\\frac{\\partial E_{tot}}{\\partial w} \\equiv \\Delta w\\$ and then I update the weights proportionally to this value: \\$w_{new} = w + \\eta \\Delta w\\$\n",
    "\n",
    "I then repeat this until the total error \\$E_{tot}=\\sum_p E_p\\$ (where \\$E_p=\\frac{1}{2}\\sum_{k=1}^K(d_k-o_k)^2\\$) is below a certain threshold (chosen by me at the beginning).\n",
    "\n",
    "### Calculation:\n",
    "\n",
    "Let's find \\$\\Delta w\\$. \\$\\Delta w = -\\frac{\\partial E_{tot}}{\\partial w} = -\\sum_p\\frac{\\partial E_{p}}{\\partial w} \\equiv \\sum_p\\Delta_pw\\$\n",
    "\n",
    "Then, for a generic \\$w_{tu}\\$ (the connection of a generic unit \\$o_t\\$ from a generic unit \\$o_u\\$ we have \\$\\Delta_pw_{tu} = - \\frac{\\partial E_{p}3}{\\partial w_{tu}} = -\\frac{\\partial E_{p}}{\\partial net_t}  \\cdot \\frac{\\partial net_t}{\\partial w_{tu}} = \\delta_t \\cdot o_u\\$\n",
    "\n",
    "Let's look at these two terms:\n",
    "- The second term comes from the explicit calculation using the definition of _net_: \\$net_t = \\sum_s w_{ts}o_s \\implies \\frac{\\partial net_t}{\\partial w_{tu}} = \\frac{\\partial \\sum_s w_{ts}o_s}{\\partial w_{tu}} = o_u\\$ (all the other terms equal 0)\n",
    "- Expanding on the first term: \\$\\delta_t = - \\frac{\\partial E_p}{\\partial net_t} = - \\frac{\\partial E_p}{\\partial o_t} \\cdot \\frac{\\partial o_t}{\\partial net_t} = -\\frac{\\partial E_p}{\\partial o_t} \\cdot f_t^\\prime(net_t)\\$, where we used \\$o_t = f_t(net_t)\\$\n",
    "\n",
    "\n",
    "So we have gotten to the point where we have an expression for \\$-\\frac{\\partial E_p}{\\partial w_{tu}} = -\\frac{\\partial E_p}{\\partial o_t} \\cdot f_t^\\prime(net_t) \\cdot o_u\\$. All that remains is to calculate \\$-\\frac{\\partial E_p}{\\partial o_t}\\$. First, let's remember that \\$E_p = \\frac{1}{2}\\sum_{k=1}^K(d_k-o_k)^2\\$.\n",
    "We must derive this in \\$o_t\\$, which gets seperated into two cases, based on what type of unit \\$o_t\\$ is:\n",
    "\n",
    "1. If \\$o_t\\$ is an output unit (\\$t=k\\$) then \\$-\\frac{\\partial E_p}{\\partial o_k} = - \\frac{\\partial \\frac{1}{2} \\sum_{r=1}^K(d_r-o_r)^2}{\\partial o_k} = (d_k-o_k)\\$. Therefore, we arrive to \\$\\boxed{\\delta_k = (d_k-o_k)\\cdot f_t^\\prime(net_t)}\\$\n",
    "2. If \\$o_t\\$ is a hidden layer unit (\\$t=j\\$) then \\$-\\frac{\\partial E_p}{\\partial o_j} = \\sum_{k=1}^K-\\frac{\\partial E_p}{\\partial net_k} \\cdot \\frac{\\partial net_k}{\\partial o_j} = \\sum_{k=1}^K \\delta_k \\cdot w_{kj}\\$ Since \\$\\frac{\\partial net_k}{\\partial o_j} = w_{kj}\\$ (for the same reason as discussed before, applying the derivative to the definition and cancelling out all constant terms in the sum) and we know that \\$-\\frac{\\partial E_p}{\\partial net_k} = (d_k-o_k)\\cdot f^\\prime_k(net_k) \\$ (rearrange the formula found in point 1.) we arrive to \\$\\boxed{\\delta_j=\\left(\\sum_{k=1}^K\\delta_k\\cdot w_{kj}\\right)\\cdot f^\\prime_j(net_j)}\\$\n",
    "\n",
    "So now we find ourself in the following situation: \\$\\Delta_pw_{tu}=\\delta_t \\cdot o_u\\$ where we have the seperate expressions for \\$\\delta_t\\$ based on what type of unit \\$o_t\\$ is, and so \\$w_{tu}^{new}=w_{tu} + \\eta\\cdot\\delta_t\\cdot o_u\\$\n",
    "\n",
    "\n",
    "### Summray:\n",
    "This calculation was done considering one hidden layer connected directly to the output layer, but there can be several hidden layers that propagate between the layers they are connected to. There is no change in the derivation.\n",
    "\n",
    "We have (in order):\n",
    "1. Forward computation (computation of unit outputs)\n",
    "2. Computation of errors and delta at the output layer level\n",
    "3. Propagation of the deltas backwards from the output layer to the hidden layers\n",
    "4. Updating of the weights\n",
    "5. Repeat\n",
    "\n",
    "This computation and update must be applied also to all the __bias terms__ of the units.\n",
    "\n",
    "The cost of this algorithm is proportional to the number of weights, not the square of the weights (compute one \\$\\delta_t\\$ for all \\$w_{tu}\\$ which allows to not have to do one calculation for each _(t,u)_).\n",
    "\n",
    "Back-propagation is __fundamental__ for almost any NN learning algorithm. \n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df2bcb6-4dc8-4772-9d8e-0c91a2833dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
