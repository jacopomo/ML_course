{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b5a7f0-5af9-4a8b-a108-02a24b69625e",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "## Jacopo Omodei e Michelangelo Leoni\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc687a8-0d77-412d-9d18-8b5c6c96829e",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "[Intro](#intro)\n",
    "\n",
    "[Linear](#linear)\n",
    "\n",
    "[Knn](#knn)\n",
    "\n",
    "[Neural Networks](#NN)\n",
    "\n",
    "-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c7cc22b-61fc-4912-8451-195684dea0d2",
   "metadata": {},
   "source": [
    "# Introduction (lezioni 1-4)\n",
    "\n",
    "<a id=\"intro\"></a>\n",
    "\n",
    "Loss: how do we measure the quality of the approximation?\n",
    "We produce an h(x) value (model evaluarted at input point x), and we want to measure the distance between h(x) and d (known input target for x). \n",
    "We use an \"inner\" loss \\$L(h_w(x),d)$, which we would like to be small. \n",
    "\n",
    "The _Error_ (or _Risk_ or _Loss_) is an expected value of this _L_ (a \"sum\" or mean of the inner loss _L_ over the set of samples)\n",
    "\\$Loss(h_w)=E_{rror}(w)=\\frac{1}{l}\\sum^l_{p=1}L(h_w(x_p),d_p)$\n",
    "\n",
    "Def: __Regression__: _predicting a numerical value_\n",
    "- __Oputput__: \\$d_p=f(x_p)+e$ (real value function + random error)\n",
    "- __H__: a set of real-valued functions\n",
    "- __Loss function__ _L_: measures the approximation accuraccy/error\n",
    "- A common loss function for regression: the squared error:\n",
    "\n",
    "\\$L(h_w(x_p),d_p)=(d_p-h_w(x_p))^2$\n",
    "The mean over the dataset provides the MSE\n",
    "\n",
    "Def: __Classification__: slide 57 2-3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Loss:\n",
    "Loss function: funzione dentro\n",
    "\n",
    "Error: somma sui dati della loss function\n",
    "\n",
    "Loss: Il valore di aspettazione della loss function (Error / numero data). Ci possono essere altri termini aggiunti che vogliamo minimizzare ma che non sono da considerare come errori sui dati. Essendo la funzione che voglio minimizzare, è usata durante il training, mentre l'errore per quando vuoi valutare come va il modello.\n",
    "\n",
    "Empirical risk (what we minimize, empirical risk minimization ERM), also called _training error E_: \\$R_{emp}=R_{emp}(h,TR)=\\frac{1}{l}\\sum^l_{p=1}L(h_w(x_p),d_p)$. \n",
    "\n",
    "The idea is that we are using \\$R_{emp}$ to approximate R\n",
    "\n",
    "## Vapnik-Chervonenkis-dim and SLT:\n",
    "- Given the _VC-dim_, a measure of the complexity of _H_ (flexibility to fit data)\n",
    "\n",
    "\\$R \\leq R_{emp} + \\varepsilon(1/l,dim(VC),1/\\delta)$\n",
    "\n",
    "where l is the dimension of the dataset, and delta is the confidence level. The term that is added to the empirical R (so, epsilon) is called the VC-confidence.\n",
    "\n",
    "![Vapnik-Chervonenkis_diagram](figures/Vapnik-Chervonenkis_diagram.png)\n",
    "\n",
    "__SLT__: Statistical learning theory. It allows formal framing of the problem of generalization and overfitting, providing \n",
    "analytic upper-bound  to the risk R for the prediction over all the data, regardless to \n",
    "the type of learning algorithm or details of the model\n",
    "\n",
    "## Validation\n",
    "After the model has trained you can use it to evaluate how good the model was on the validation data (to make choices about model selection) or to estimate its prediction/generalization error on new test data. Either way, it returns an estimation\n",
    "\n",
    "Partition data set D into three parts. _training set_ TR, _validation/selection set_ VL, _test set_ TS.\n",
    "- All three of the sets are disjointed and serve their own function\n",
    "- __TR__ is used for the training algorithm\n",
    "- __VL__ is used to select the best model (like tuning hyper-parameters)\n",
    "- __TS__ is not to be used for tuning but instead to assess the model.\n",
    "\n",
    "__K-fold__ cross validation: we will see in the future __LINK__\n",
    "\n",
    "-------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0aacce-421c-48e8-9c91-d3d135e87537",
   "metadata": {},
   "source": [
    "# Linear models (L5)\n",
    "<a id=\"linear\"></a>\n",
    "\n",
    "A linear model is a model which the input vector is weighted with linear coefficient weights:\n",
    "\\$w^Tx+w_0 = w_0 + \\sum_{i=1}^nw_ix_i$\n",
    "\n",
    "\\$w_0$ is the _intercept, threshold, bias, offset..._ Often it is convenient to include the constant x_0=1 so that we can encapsulate the bias w_0 as the first element of the vector w. Like this we get:\n",
    "\n",
    "\\$h(x_p)=x_p^Tw=\\sum_{i=0}^nw_ix_i$\n",
    "\n",
    "_w_ are the free parameters, the \"weights\".\n",
    "\n",
    "Given the data set and the linear model, we can state the learning problem as a LMS problem. We just have to find the best weights so that we have the best \\$h_w(x)$\n",
    "\n",
    "#### Bias\n",
    "- Language bias: the H is a set of linear functions (restrictive)\n",
    "- Ordered search guided by the Least Squares minimization goal \n",
    "\n",
    "## Classification\n",
    "The function \\$h(x_p)=x_p^Tw=\\sum_{i=0}^nw_ix_i$ rappresents a hyperplane in n dimensions that we can use to split our space into a positive and a negative space. __LTU__ (Linear threshold unit) gives the seperation between these two zones. In general, two groups are linearly separable in n-dimensional space if they can be separated by an (n − 1)-dimensional hyperplane, and an exact solution exists only for linearly separable groups.\n",
    "\n",
    "Tecnically it would be intuitive, at least for classification problems, to use a loss function that is equal to 1 for misclassified data and 0 for correctly classified data. However, this loss function is non-differentiable and if we want to implement gradient descent minimization we need a smooth loss function, like the squared errors loss function. A closed form solution exists, but we will opt for a recursive gradient descent technique.\n",
    "\n",
    "Binary classification loss functions (0 if classified correctly, 1 if wrong) are not useless, quite the opposite! You can't use them to train the machine as it is not differentiable, but once trained you can calculate it to see then number of errors that your model commits. We define _accuracy_ the mean of correctly classified errors \\$\\frac{l-N_{err}}{l}$.\n",
    "\n",
    "__Gradient descent technique__\n",
    "We calculate the gradient of the loss which gives us the direction to change each weight in order to decrease the loss. Therefore:\n",
    "\\$w_{new} = w + \\eta\\cdot \\Delta w \\quad \\text{with} \\quad \\Delta w = - \\frac{\\partial E(W)}{\\partial w}$\n",
    "\n",
    "\\$\\eta$ is the learning rate (step-size)\n",
    "\n",
    "### Batch vs On-line (SGD) gradient descent\n",
    "What has been discussed before is the batch gradient descent, where the loss is calculated and the gradient is taken after one epoch (all of the data has been used) has passed. Then, the gradient is evaluated and the weights are modified before the next full _batch_. \n",
    "\n",
    "The alternative is the SGD (Stochastic Gradient Descent), where after every data point the loss function is calculated and the weights are modified - so that in one epoch you have many many adjustments to the model. This can be faster, as it does not wait for a full epoch to pass before modifying _w_, but the direction of the modification depends only on the data that has already been seen. In the figure, blue is the batch descent, purple SGD and green is a mini-batch SGD (not on every point but modulated)\n",
    "\n",
    "![Batch vs SGD](figures/batch_vs_SGD.png)\n",
    "\n",
    "## Extension of the linear model and generalization\n",
    "Let's remember that for a model to be considered linear it must be linear in the parameters (w). For example, a totally legit linear model is the _polynomial regression_: \\$h_w(x) = w_0 + w_1\\cdot x + w_2\\cdot x^2 + \\cdots + w_n\\cdot x^n = \\sum_{i=0}^nw_ix^i$\n",
    "\n",
    "__Linear basis expansion (LBE)__: \\$h_w(x)=\\sum_{k=0}^Kw_k\\phi_k(x)$. \n",
    "\n",
    "The phi functions are generic, and so we have the usual sum of \\$w_ix_i$ for the _n_-dimension of the space, plus potential other _w's_ multiplied by functions of the input data. Therefore, in general, _K_ is greater than _n_\n",
    "\n",
    "This is fantastic because it allows us more flexibility in our model but risks the _curse of dimensionality_, where the volume of the problem space increases so fast that the available data become sparse, the data became no more sufficient to support the model complexity. Also, the \\$\\phi$ are fixed before observing the training data (otherwise it would be an adaptive model like a NN).\n",
    "\n",
    "__Ridge regression: (Tikhonov regularization):__\n",
    "There is a way to combat the  _curse of dimensionality_, penalizing models with high values of _|w|_. I add a term that is proportional to the modulus squared (\\$ ||w||^2=\\sum w_j^2$) to the loss. \n",
    "\n",
    "\\$Loss(w) = \\sum_{p=1}^l(y_p-x_p^Tw)^2+\\lambda ||w||^2$. Lambda (regulatization hyper-parameter) is a small positive value. The second term of this loss function is called the regularization (penalty) term.\n",
    "\n",
    "In the gradient descent formularization, we get \\$w_{new} = w + \\eta\\cdot \\Delta w - 2 \\lambda \\cdot w$. As we can see, it is a _weight decay technique, which \"pulls\" all of the w's towards 0. Since you are reducing the weights overall, you are also simplyifing the model (reducing the VC-dim).\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4b876-4821-4c8a-a1bf-03f452512900",
   "metadata": {},
   "source": [
    "# KNN\n",
    "<a id=\"knn\"></a>\n",
    "\n",
    "__Timing__\n",
    "- Eager: Analyze the training data and construct an explicit hypothesis\n",
    "- Lazy: Store the training data and wait until a test data point is presented, then construct an ad hoc hypothesis to classify that one data point.\n",
    "\n",
    "So far we have seen eager models, where the full \\$h(x)$ was found by looking at all of the data, wheras k-nn finds the best classification point by point.  \n",
    "\n",
    "## k-nn algorithm:\n",
    "- Store the training data\n",
    "- Given an input x find the nearest training example \\$x_i\\$\n",
    "- Then output \\$y_i$\n",
    "\n",
    "Find _i_ so that we have \\$\\min d(x,x_i) \\rightarrow i(x) = arg_p \\min d(x,x_p) \\quad \\text{using Euclidean distance} \\quad d(x,x_p)=\\sqrt{\\sum_{t=1}^n(x_t-x_{p,t})^2}=||x-x_p||$ \n",
    "\n",
    "k-nn refers to how many neighbors are considered in the nearest neighbor evaluation. For example, the one above is a 1-nn algorithm.\n",
    "\n",
    "A natural way to classify a new point is to have a look at its k-neighbors and take the average. \n",
    "\n",
    "If there is a clear dominance of one of the classes in the neighborhood of an observation x, then it is likely that the observation itself would belong to that class, too. Thus the classification rule is the majority voting among the members of that neighborhood. This extends naturally to multi-class classification where you look at the class of the majority of your neighbors. \n",
    "\n",
    "You can also create an algorithm that gives more weight in the nearest neighbor calculation to the closest point. For example, you can multiply by a factor \\$ \\propto \\frac{1}{d^2}\\$. \n",
    "\n",
    "## Algorithm flexibility\n",
    "\n",
    "The k-nn algorithm has _k_ parameters, but \\$\\frac{l}{k}\\$ _effective degrees of freedom_. This number is roughly equal to the number of k-point neighborhoods and can be used to estimate the flexibility of the model. If _k_ is 1, then teh model has no training error, and is in _overfitting_. For \\$k=l\\$, the model is obviously in _underfitting_ since the model outputs a constant value for all inputs. \n",
    "\n",
    "![k-nn parameters](figures/knn_parameters.png)\n",
    "\n",
    "The red curves are test and the green are training error for k-nn classification (changing K). The results for linear regression are the bigger green and red squares at three degrees of freedom. The purple line is the _optimal Bayes Error Rate_.\n",
    "\n",
    "__Bayes error rate__:\n",
    "Bayes optimal solution (called Bayes classifier): the minimum achievable error rate given the distribution of the data. Only calculatable if you know the initial distribution. K-nn models approach this limit quite well for a certain range of _k's_.\n",
    "\n",
    "__Inductive bias__: type of distance used (Euclidean, etc.) \n",
    "\n",
    "Limitations: \n",
    "\n",
    "-  Computationally intensive (in time) for each new input: computing the distances from the test sample to _all_ stored vectors local approximations and in space because you must memorize the full training set.\n",
    "- Curse of Dimensionality: increasing the dimension causes a loss of density in the training space. If you need _l_ points to sufficiently sample a 1-dimensional space, then you will need \\$l^n\\$ points to obtain a similar sampling density in an _n-dimensional_ space.\n",
    "- Curse of Noisy:  if the target depends on only few of many features in x we could retrieve a “similar pattern” with the similarity  dominated by the large number of irrelevant features.\n",
    "- Different data ranges correspond to certain features being favored.\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e84173-c1a7-44c1-980e-c279d65f5390",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "<a id=\"NN\"></a>\n",
    "\n",
    "## Basic idea and definitions:\n",
    "Complex behavior emerging from interaction of simple computational units. \n",
    "\n",
    "A __node__ (or neuron, unit) takes inputs from external sources or other units. Each input connection has a certain __weight__ _w_, which will be the free parameters we will modify throughout the learning process (synaptic strength in real neurons).\n",
    "\n",
    "The weighted sum \\$net_i(x)=\\sum_jw_{ij}x_j\\$ is called the __net input__ to unit _i_. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \\(w_{ij}\\) refers to the weight of the input j onto the unit i\n",
    "</div>\n",
    "\n",
    "The function _f_ is the unit's __activation function__, and the output of the unit _i_ is \\$o_i(x)=f(net_i(x))\\$ \n",
    "\n",
    "![neuron_diagram](figures/neuron_diagram.png)\n",
    "\n",
    "## Activation functions:\n",
    "\n",
    "- Linear: \\$o_i(x)= \\sum_iw_ix_i\\$, so the output is just the net input.\n",
    "- Threshold: \\$o_i=sign(net_i(x))\\$. The unit either has an output or no output. \n",
    "- Sigmoid: \\$\\frac{1}{1+e^{-net_i(x)}}\\$ _sigmoid_ logistic function (a smoother 0-1 activation)\n",
    "- LTU (Linear threshold unit): 0 for \\%x<0\\% and linear for \\$x>0\\$.\n",
    "\n",
    "## Perceptron:\n",
    "Frank Rosenblatt (1957), biologically inspired model for a neural network, where every unit is very simple and the connections between them produce a more complex outcome\n",
    "\n",
    "__History: McCulloch & Pitts Networks:__\n",
    "Neurons are in two possible states: 1 or 0. All connections are equivalent and characterized by a real number (w). A neuron becomes active when the weighted sum of the connections + a bias is greater than 0 (threshold activation function). Therefore, each node can give a binary output, and so it is useful for binary classification tasks.\n",
    "\n",
    "With only two levels of interconnected nodes you can represent any Boolean function. With enough layers, perceptrons can solve any linearly seperatable problem. \n",
    "\n",
    "__Perceptron convergence theorem__: The perceptron is guaranteed to converge (classifying all the input patterns correctly) in a finite number of steps if the problem is linearly separable independently of the starting point. The final solution is not unique and it depends on the starting point. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The lecture [slides](../slides/L7_Neural_Networks.pdf)\n",
    " present a thorough proof of this theorem, which will not be treated in this document\n",
    "</div>\n",
    "\n",
    "No sort of learning algorithm proposed yet, just a way to solve a problem using principles inspired by nature. \n",
    "\n",
    "## Learning algorithms:\n",
    "We will start by looking at the Perceptron learning alg. proposed by Rosenblatt (though it is not the only one that exists). We have non-linear units during training, with a hard limiter on threshold activation function. It is only useful for classification studies. \n",
    "\n",
    "Its goal is to minimize the number of misclassified patterns. \n",
    "\n",
    "1.  Initialize the weights (either to zero or to a small random value)\n",
    "2.  Pick a learning rate \\$\\eta\\$ between 0 and 1\n",
    "3.  Loop until stopping condition (weights don't change - converged to a solution) is met\n",
    "\n",
    "For each training pattern (x,d), where d is +1 or -1\n",
    "\n",
    "- Compute the activation function \\$ out = sign(w^Tx) \\$ (equal to \\$ \\pm1 \\$)\n",
    "- If \\$out = d\\$ don't change the weights\n",
    "- If \\$out \\neq d\\$ update the weights \\$ w_{new} = w + \\frac{\\eta}{2} \\cdot (d-out)x\\$ (_Hebbian learning_) where \\$\\delta = (d-out)\\$\n",
    "\n",
    "This is an error correction rule, where the weights are updated proportionally to the error.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e38948-7368-4495-9f48-1b2a0ae25913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
